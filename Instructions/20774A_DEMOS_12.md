# Module 12: Using Machine Learning with HDInsight

- [Module 12: Using Machine Learning with HDInsight](#module-12-using-machine-learning-with-hdinsight)
    - [Demo 1: Provisioning a Hadoop cluster on HDInsight](#demo-1-provisioning-a-hadoop-cluster-on-hdinsight)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Start a cluster configuration](#start-a-cluster-configuration)
        - [Configure basic cluster details](#configure-basic-cluster-details)
        - [Configure cluster storage](#configure-cluster-storage)
        - [Configure cluster size and create the cluster](#configure-cluster-size-and-create-the-cluster)
    - [Demo 2: Managing a Hadoop cluster on HDInsight](#demo-2-managing-a-hadoop-cluster-on-hdinsight)
        - [Scenario](#scenario-1)
        - [Preparation](#preparation-1)
        - [View the cluster dashboard](#view-the-cluster-dashboard)
        - [Connect to the cluster using SSH](#connect-to-the-cluster-using-ssh)
        - [Use SSH to browse HDFS](#use-ssh-to-browse-hdfs)
    - [Demo 3: Working with HDInsight](#demo-3-working-with-hdinsight)
        - [Scenario](#scenario-2)
        - [Preparation](#preparation-2)
        - [Use the Python Spark shell](#use-the-python-spark-shell)
        - [Run a MapReduce job](#run-a-mapreduce-job)

## Demo 1: Provisioning a Hadoop cluster on HDInsight

### Scenario

In this demonstration, you will see how to:
-  Start a cluster configuration.
-  Configure basic cluster details.
-  Configure cluster storage.
-  Configure cluster size and create the cluster.

### Preparation

Ensure that the **MT17B-WS2016-NAT**, **20774A-LON-DC**, and **20774A-LON-DEV** virtual machines are running, and that you are logged on to **20774A-LON-DEV** as **ADATUM\AdatumAdmin**.

### Start a cluster configuration

1.  On the **20774A-LON-DEV** virtual machine, on the Start menu, start to type **Internet Explorer**, and then click **Internet Explorer**.
2.  In the address bar, type **http://azure.microsoft.com**, click **Portal**, and then sign in using the Microsoft account that is associated with your Azure Learning Pass subscription.
3.  In the **Azure Portal**, click **All resources**, and then verify that there are no existing HDInsight clusters in your subscription. 
4.  In the **Azure Portal**, in the left pane, click **+ Create a resource**.
5.  Click **Analytics**, and then click **HDInsight**.

### Configure basic cluster details

1.  On the **HDInsight** blade, type the following details, and then click **Cluster type**:
    -  **Cluster name**: \<*your name\>\<date*\>hdi
2.  Enter the following details, and then click **Select**:
    -  **Cluster type**: Hadoop
    -  **Operating system**: Linux
    -  **Version**: Hadoop 2.7.3 (HDI 3.6)
3.  Enter the following details, and then click **Next**:
    -  **Cluster login username**: hadmin
    -  **Cluster login password**: Pa55w.rdPa55w.rd
    -  **Secure Shell (SSH) username**: sadmin
    -  **Resource group (Create new)**: hadooprg
    -  **Location**: East US 2

### Configure cluster storage

1.  On the **Storage** blade, under **Select a Storage account**, click **Create new**.
2.  In the **Create a new Storage account** box, type **\<*your name\>\<date*\>sa**.
3.  In the **Default container** box, replace the suggested name with the name of your cluster (for example, **\<*your name\>\<date*\>hdi**).
4.  Leave all other settings at their defaults, and then click **Next**.

### Configure cluster size and create the cluster

1.  On the **Cluster summary** blade, next to **Cluster size**, click **Edit**.
2.  On the **Cluster size** blade, in the **Number of Worker nodes** box, type **1**.
3.  Click **Worker node size**.
4.  On the **Choose your node size** blade, click **View all**, click **A3 General Purpose**, and then click **Select**.
5.  Click **Head node size**.
6.  On the **Choose your node size** blade, click **View all**, click **A3 General Purpose**, and then click **Select**.
7.  On the **Cluster size** blade, click **Next**.
8.  On the **Script Actions** blade, click **Next**.
9.  On the **Cluster summary** blade, note the estimated cost per hour of this cluster; to avoid using up your Azure Pass allowance, it is important that you remove the cluster when you are not using it.
10. On the **Cluster summary** blade, click **Create**.
11. The deployment might take 20–30 minutes to complete. Do not wait for the cluster to be provisioned; instead, explain that you will return to this deployment during the next module. Do not continue with this exercise until the status shows as **Running**.

## Demo 2: Managing a Hadoop cluster on HDInsight

### Scenario

In this demonstration, you will see how to:
-  View the cluster dashboard.
-  Connect to the cluster using SSH.
-  Use SSH to browse HDFS.

### Preparation

Ensure that the **MT17B-WS2016-NAT**, **20774A-LON-DC**, and **20774A-LON-DEV** virtual machines are running, and that you are logged on to **20774A-LON-DEV** as **ADATUM\AdatumAdmin**.

### View the cluster dashboard

>**Note**: The commands in this demo are case-sensitive. They can be copied from **E:\Demofiles\Mod12\Demo2Cmds.txt** and pasted by right-clicking the SSH console window.

1.  On the **20774A-LON-DEV** virtual machine, ensure that you are logged in as **ADATUM\AdatumAdmin**.
2.  In Internet Explorer®, in the Microsoft Azure Portal, click **All resources**, and then click **\<*your name\>\<date*\>hdi**.
3.  On the **HDInsight Cluster** blade, check that the status shows as **Running**, and then view the summary information for your cluster. 
4.  On the **HDInsight Cluster** blade, under **Settings**, click **Cluster size**, and note that you can dynamically scale the number of worker nodes to meet processing demand.
5.  On the **Cluster size** blade, click **Overview**.
6.  In the **Cluster dashboards** section, click **Ambari home**.
7.  When you are prompted, log in as **hadmin** with a password of **Pa55w.rdPa55w.rd**.
8.  Explore the dashboard for your cluster. The dashboard is an Apache Ambari web application in which you can view and configure settings for the Hadoop services running in the cluster.
9.  When you are finished, close its tab and return to the Azure Portal tab.

### Connect to the cluster using SSH

1.  In the Microsoft Azure Portal, under **Settings**, click Secure **SSH + Cluster login**. 
2.  On the **SSH + Cluster login** blade, in the **Hostname** list, click **\<*your name\>\<date*\>-ssh.azurehdinsight.net**, and then click the **Click to copy** button. 
3.  Open a Windows command prompt.
4.  At the Command Prompt, type the following command, and then press Enter:
    ````
    putty
    ````
5.  In the **PuTTY Configuration** window, on the **Session** page, paste the host name from step 2 into the **Host Name** box.
6.  In the **Host Name** box, remove the **ssh** text at the begining.
7.  Under **Connection type**, select **SSH**, and then click **Open**. 
8.  If a security warning that the host certificate cannot be verified is displayed, click **Yes** to continue.
9.  When you are prompted, enter **Pa55w.rdPa55w.rd** as the password. 

### Use SSH to browse HDFS 

1.  In the SSH console window, type the following command to view the contents of the root folder in the HDFS file system:
    ````
    hdfs dfs -ls /
    ````
2.  Type the following command to view the contents of the **/example** folder in the HDFS file system. This folder contains subfolders for sample apps, data, and JAR components:
    ````
    hdfs dfs -ls /example
    ````
3.  Type the following command to view the contents of the **/example/data/gutenberg** folder, which contains sample text files:
    ````
    hdfs dfs -ls /example/data/gutenberg
    ````
4.  Type the following command on one line to view the text in the **ulysses.txt** file:
    ````
    hdfs dfs -text /example/data/gutenberg/ulysses.txt
    ````
5.  Note that the file contains large volumes of unstructured text.


## Demo 3: Working with HDInsight

### Scenario

In this demonstration, you will see how to:
-  Use the Python Spark shell.
-  Run a MapReduce job.

### Preparation

Ensure that the **MT17B-WS2016-NAT**, **20774A-LON-DC**, and **20774A-LON-DEV** virtual machines are running, and that you are logged on to **20774A-LON-DEV** as **ADATUM\AdatumAdmin**.

### Use the Python Spark shell

>**Note**: The commands in this demo are case-sensitive. They can be copied from **E:\Demofiles\Mod12\Demo3Cmds.txt** and pasted by right-clicking the SSH console window.

1.  In the SSH console window, launch the pyspark shell by typing the following command, and then press Enter:
    ````
    pyspark
    ````
2.  Point out the Spark log appearing with a **>>>** prompt.
3.  The pyspark shell will give you the Spark context by default.
4.  Explain that you will now create an RDD for a text file.
5.  Type the following command, and then press Enter:
    ````
    txtRdd = sc.textFile('/example/data/gutenberg/ulysses.txt')
    ````
6.  Explain that you will now call an action method on the RDD to count the number of records in the RDD by calling the following function:
    ````
    txtRdd.count()
    ````
7.  Point out that you can do simple transformations, such as filtering lines that have the word **good** in the lines, by calling a transformation API such as **filter** before you call an Action API **count()**:
    ````
    txtRdd = txtRdd.filter(lambda x: 'good' in x)
    txtRdd.count()
    ````
8.  Type the following command, and then press Enter:
    ````
    quit()
    ````

### Run a MapReduce job

1.  In the SSH console window, type the following command, and then press Enter:
    ````
    ls /usr/hdp/current/hadoop-mapreduce-client
    ````
2.  Point out the sample JAR files stored in the cluster head node.
3.  Type the following command, and then press Enter: 
    ````
    hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar
    ````
4.  Point out the list of MapReduce functions in the hadoop-mapreduce-examples.jar file.
5.  Type the following command, and then press Enter: 
    ````
    hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount
    ````
6.  Point out the help text for the **wordcount** function.
7.  Type the following command, and then press Enter: 
    ````
    hadoop jar /usr/hdp/current/hadoop-mapreduce-client/hadoop-mapreduce-examples.jar wordcount /example/data/gutenberg/ulysses.txt /example/results
    ````
8.  Point out that this command runs a MapReduce job to process a text file (**ulysses.txt**) and store the results in the **/example/results** folder.
9.  When the MapReduce job has completed, type the following command, and then press Enter:
    ````
    hdfs dfs -ls /example/results
    ````
10. Point out that a file named **part-r-00000** has been created in the **results** folder.
11. Type the following command, and then press Enter: 
    ````
    hdfs dfs -text /example/results/part-r-00000
    ````
12. Point out the type of data in the output file.
13. Close the SSH console window.


---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.
