# Module 9: Initializing and Optimizing Machine Learning Models

- [Module 9: Initializing and Optimizing Machine Learning Models](#module-9-initializing-and-optimizing-machine-learning-models)
    - [Demo 1: Using cross-validation folds](#demo-1-using-cross-validation-folds)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Create a subset of cases from the beginning of a dataset](#create-a-subset-of-cases-from-the-beginning-of-a-dataset)
        - [Create a sample of data](#create-a-sample-of-data)
        - [Split data into partitions](#split-data-into-partitions)
        - [Use data from a predefined partition](#use-data-from-a-predefined-partition)
    - [Demo 2: Using hyperparameters](#demo-2-using-hyperparameters)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Create a tuned model](#create-a-tuned-model)
        - [Create an untuned model (for comparison)](#create-an-untuned-model-for-comparison)
        - [Compare the results](#compare-the-results)
    - [Demo 3: Evaluating an ensemble by using stacking](#demo-3-evaluating-an-ensemble-by-using-stacking)
        - [Scenario](#scenario)
        - [Preparation](#preparation)
        - [Explore a Gallery experiment that uses stacking to build an ensemble of classifiers](#explore-a-gallery-experiment-that-uses-stacking-to-build-an-ensemble-of-classifiers)
        - [Evaluate the ensemble experiment](#evaluate-the-ensemble-experiment)

## Demo 1: Using cross-validation folds

### Scenario

In this demonstration, you will see how to configure the Partition and Sample module to:
-	Create a subset of cases from the beginning of a dataset.
-	Create a sample of data.
-	Split data into partitions.
-	Use data from a predefined partition.

### Preparation

Before running this demonstration, you must complete the following steps:
1.	On the **20774A-LON-DEV** virtual machine, in Microsoft Internet Explorer®, in the address bar, type **https://studio.azureml.net/**.
2.	On the **Microsoft Azure Machine Learning Studio** page, click **Sign In**. 
3.	If you are prompted for credentials, use the details of the Microsoft account that you created for this course.

### Create a subset of cases from the beginning of a dataset

1.	In Microsoft Azure Machine Learning Studio, ensure that **EXPERIMENTS** is selected in the navigation pane, click **+ NEW**, and then click **Blank Experiment**.
2.	In the **Search experiment items** box, start to type **Adult**, and then from the module list, drag the **Adult Census Income Binary Classification dataset** module onto the experiment canvas.
3.	In the **Search experiment items** box, start to type **Partition**, and then from the module list, drag the **Partition and Sample** module onto the experiment canvas, below the dataset module.
4.	Connect the output of the dataset module to the input of the **Partition and Sample** module.
5.	Click the **Partition and Sample** module, and then in the **Properties** pane, in the **Partition or sample** mode box, select **Head**.
6.	In the **Number of rows to select** box, type **100** to set the maximum number of rows to return.
7.	Run the experiment by clicking **RUN** at the bottom of the page.
8.	Click the output port of the **Partition and Sample** module, and then click **Visualize**.
9.	Point out that the module generates a single dataset containing only the specified number of rows (100). The rows are always read from the top of the dataset.
10.	Close the visualization by clicking the **x** at the top-right of the window.

### Create a sample of data

1.	Click the **Partition and Sample** module, and then in the **Properties** pane, in the **Partition or sample mode** box, select **Sampling**.
2.	In the **Rate of sampling** box, type **0.1**. This indicates that 10 percent of the rows from the source dataset should be included in the output dataset.
3.	In the **Random seed for sampling** box, type **0**. A value of zero indicates that a new random seed will be used for each run of the model.
4.	Run the experiment by clicking **RUN** at the bottom of the page.
5.	Click the output port of the **Partition and Sample** module, and then click **Visualize**.
6.	Point out that the module’s output is now a single dataset containing a random sample of 10 percent of the rows (3,256) in the dataset.
7.	Close the visualization by clicking the **x** at the top-right of the window.
8.	Click the **Partition and Sample** module, and then in the **Properties** pane, set **Stratified split for sampling** to **True**.
9.	Click **Launch column selector**. 
10.	In the **Select a single column** dialog box, move **education-num** from the **AVAILABLE COLUMNS** list into the **SELECTED COLUMNS** list, and then click the check mark. 
11.	Run the experiment by clicking **RUN** at the bottom of the page.
12.	Click the output port of the **Partition and Sample** module, and then click **Visualize**.
13.	Point out that the module now generates a single dataset that contains a representative sampling of the data based on the values of the **education-num** column.
14.	Close the visualization by clicking the **x** at the top-right of the window.

### Split data into partitions

1.	Click the **Partition and Sample** module, and then in the **Properties** pane, in the **Partition or sample mode** box, select **Assign to Folds**.
2.	Confirm that the property values are set as follows:
    -	Use replacement in the partitioning: **not selected**
    -	Randomized split: **selected**
    -	Random seed: **0**
    -	Specify the partitioner method: **Partition evenly**
    -	Specify number of folds: **5**
    -	Stratified split: **false**
3.	Run the experiment by clicking **RUN** at the bottom of the page.
4.	Click the output port of the **Partition and Sample** module, and then click **Visualize**.
5.	Point out that the module generates a single dataset partitioned into five folds. Explain that the fold assignments cannot be viewed directly; they are held only in the dataset metadata.
6.	Close the visualization by clicking the x at the top-right of the window.
7.	Click the **Partition and Sample** module, and then in the **Properties** pane, in the **Specify the partitioner method** list, select **Partition with customized proportions**.
8.	In the **List of proportions separated by comma** box, type **0.1,0.1,0.5**.
9.	Run the experiment by clicking **RUN** at the bottom of the page.
10.	Explain that, with these settings, the module generates a single dataset partitioned into four folds. Explain that the fourth fold is automatically generated because the sum of values in the proportions list is less than 1.
11.	Close the visualization by clicking the **x** at the top-right of the window.

### Use data from a predefined partition

1.	In the **Search experiment items** box, start to type **Partition**, and then from the module list, drag the **Partition and Sample** module onto the experiment canvas, below the existing **Partition and Sample** module.
2.	Connect the input of the second **Partition and Sample** module to the output of the first **Partition and Sample** module. Ensure that the first **Partition and Sample** module is still configured using the **Assign to Folds** option from the previous steps.
3.	Click the second **Partition and Sample** module, and then in the **Properties** pane, in the Partition or sample mode list, click **Pick Fold**.
4.	In the **Specify which fold to be sampled from** box, type **2** to select the second fold. Explain that you could attach multiple instances of **Partition and Sample** in **Pick Fold** mode to the first instance, each picking a different fold.
5.	Run the experiment by clicking **RUN** at the bottom of the page.
6.	Click the output port of the second **Partition and Sample** module, and then click **Visualize**.
7.	Point out that the module generates a single dataset that contains only the rows (3,256) allocated to the second fold.
8.	Close the visualization by clicking the **x** at the top-right of the window.

## Demo 2: Using hyperparameters

### Scenario

In this demonstration, you will see how to:
-	Create a tuned model.
-	Create an untuned model (for comparison).
-	Compare the results.

### Preparation

Before running this demonstration, you must complete the following steps:
1.	On the **20774A-LON-DEV** virtual machine, in Microsoft Internet Explorer®, in the address bar, type **https://studio.azureml.net/**.
2.	On the **Microsoft Azure Machine Learning Studio** page, click **Sign In**. 
3.	If you are prompted for credentials, use the details of the Microsoft account that you created for this course.

### Create a tuned model

1.	In Machine Learning Studio, ensure that **EXPERIMENTS** is selected in the navigation pane, click **+ NEW**, and then click **Blank Experiment**.
2.	In the **Search experiment items** box, start to type **Adult**, and then from the module list, drag the **Adult Census Income Binary Classification dataset** module onto the experiment canvas.
3.	In the **Search experiment items** box, start to type **Split**, and then from the module list, drag the **Split Data** module onto the experiment canvas, below the dataset module.
4.	Connect the output of the dataset module to the input node of the **Split Data** module.
5.	Click the **Split Data** module, and then in the **Properties** pane, in the **Fraction of rows in the first output dataset** box, type **0.7**.
6.	Explain that this will create one set that contains 70 percent of the data and another that contains 30 percent of the data. The 70 percent portion of the data will be used for training.
7.	In the **Search experiment items** box, start to type **Vector**, and then from the module list, drag the **Two-Class Support Vector Machine** module onto the experiment canvas, to the left of the **Split Data** module.
8.	Click the **Two-Class Support Vector Machine** module, and then in the **Properties** pane, configure the following properties:
    -	Create trainer mode: **Parameter range**
    -	Use Range Builder: **Selected**
    -	Parameter range: **1-100**
    -	Number of points: **4**

    Leave all other values at their defaults.
9.	In the **Search experiment items** box, start to type **Split**, and then from the module list, drag a second **Split Data** module onto the experiment canvas, below the existing **Split Data** module.
10.	Click the second **Split Data** module, and then in the **Properties** pane, in the **Fraction of rows in the first output dataset** box, type **0.6**.
11.	Connect the left output of the first **Split Data** module to the input of the new **Split Data** module.
12.	In the **Search experiment items** box, start to type **Tune**, and then from the module list, drag the **Tune Model Hyperparameters** module onto the experiment canvas, below the **Two-Class Support Vector Machine** and second **Split Data** modules.
13.	Click the **Tune Model Hyperparameters** module, and then in the **Properties** pane, in **Specify parameter sweeping** mode, click **Entire grid**.
14.	Click **Launch column selector**.
15.	In the text box, type **income**, press Enter, and then click the check mark. 
Explain that **income** is the value that the model will attempt to predict.
16.	Connect the output of the **Two-Class Support Vector Machine** module to the left input of the **Tune Model Hyperparameters** module. 
17.	Connect the left output of the second **Split Data** module to the center input of the **Tune Model Hyperparameters** module. 
18.	Connect the right output of the second **Split Data** module to the right input of the **Tune Model Hyperparameters** module.
19.	In the **Search experiment items** box, start to type **Score**, and then from the module list, drag the **Score Model** module onto the experiment canvas, below the **Tune Model Hyperparameters** module.
20.	Connect the right output of the **Tune Model Hyperparameters** module (the trained best model) to the left input of the **Score Model** module. 
21.	Connect the right output of the first **Split Data** module (the test dataset) to the right input of the **Score Model** module.
22.	In the **Search experiment items** box, start to type **Evaluate**, and then from the module list, drag the **Evaluate Model** module onto the experiment canvas, below the **Score Model** module.
23.	Connect the output of the **Score Model** module to the left input of the **Evaluate Model** module.

### Create an untuned model (for comparison)

1.	In the **Search experiment items** box, start to type **Train**, and then from the module list, drag the **Train Model** module onto the experiment canvas, to the right of the **Tune Model Hyperparameters** module. 
2.	Connect the left output of the second **Split Data** module (the one that is already connected to the **Tune Model Hyperparameters** module) to the right input of the **Train Model** module. 
3.	Click the **Train Model** module, and then click **Launch column selector**.
4.	In the text box, type **income**, press Enter, and then click the check mark.
5.	In the **Search experiment items** box, start to type Vector, and then from the module list, drag a second Two-Class Support Vector Machine module onto the experiment canvas, to the right of the second **Split Data** module and above the **Train Model** module.
6.	Connect the output of the second **Two-Class Support Vector Machine** module to the left input of the **Train Model** module.
7.	In the **Search experiment items** box, start to type **Score**, and then from the module list, drag the **Score Model** module onto the experiment canvas, below the **Train Model** module.
8.	Connect the output of the **Train Model** module to the left input of the new **Score Model** module. 
9.	Connect the right output of the second **Split Data** module (the one that is already connected to the **Tune Model Hyperparameters** module) to the right input of the new **Score Model** module.
10.	Connect the output of the new **Score Model** module to the left input of the **Evaluate Model** module.

### Compare the results

1.	Run the experiment by clicking **RUN** at the bottom of the page.
2.	When the experiment has finished running, click the output port of the **Evaluate Model** module, and then click **Visualize**.
3.	Explain that on the receiver operating characteristic (ROC) chart, the **Scored dataset** line represents the left input of the **Evaluate Model** module and uses tuned parameters. Point out that in this example, the tuned parameters produced better accuracy than the untuned model (the right input, the **Scored dataset to compare** line). This is indicated by the ROC line being closer to the top-left of the chart.
4.	Close the visualization by clicking the **x** at the top-right of the window.

>**Note:** This demonstration covers the use of the **Tune Model Hyperparameters** module to select the best set of hyperparameters for a classification model, by comparing the results achieved by the **Tune Model Hyperparameters** module with the results of training without parameter sweeping.

## Demo 3: Evaluating an ensemble by using stacking

### Scenario

In this demonstration, you will see how to:
-	Explore a Gallery experiment that uses stacking to build an ensemble of classifiers.
-	Evaluate the ensemble experiment.

### Preparation

Before running this demonstration, you must complete the following steps:
1.	On the **20774A-LON-DEV** virtual machine, in Internet Explorer, in the address bar, type **https://studio.azureml.net/**.
2.	On the **Microsoft Azure Machine Learning Studio** page, click **Sign In**. 
3.	If you are prompted for credentials, use the details of the Microsoft account that you created for this course.
4.	In Internet Explorer, click + to create a new tab, and then navigate to **https://gallery.cortanaintelligence.com/Experiment/Building-Ensemble-of-Classifiers-using-Stacking-2**.
5.	Click **Open in Studio**.
6.	In the **Copy experiment from Gallery** dialog box, select the region associated with your workspace, select your workspace, and then click the check mark.
7.	After the model has downloaded and saved, run the experiment by clicking **RUN** at the bottom of the page.
8.	Wait for the experiment to finish running. This may take over an hour.

### Explore a Gallery experiment that uses stacking to build an ensemble of classifiers

1.	Start by explaining that you have downloaded an experiment from the Microsoft Cortana® Intelligence Gallery called “Building Ensemble of Classifiers using Stacking.”
This is a complex experiment, so to save time, you have already run the experiment.
2.	In Machine Learning Studio, zoom into the diagram and point out the initial datasets. Point out the data cleansing steps used in this model.
3.	Explain that the right part of the model is where the classifiers are run independently. Point out that this portion of the model consists of fewer modules than the left part of the model (this is most obvious if you zoom out). 
4.	Click each of the four classifiers in the right part of the model and show their parameters.
5.	In the left part of the model, show that the same four classifiers appear, and that they have the same parameters.
6.	Point to the second instance of the **Two-Class Logistic Regression** module in the left side of the model. Explain that this is the ensemble classifier, which is used to generate a result based on the outputs of the four base classifiers.

### Evaluate the ensemble experiment

1.	Click the left output of the final **Execute R Script** module at the bottom of the experiment, and then click **Visualize**.
2.	Point out the optimal AUC metrics selected by each of the base classifiers.
3.	Point out the optimal AUC metric selected by the ensemble classifier.
4.	Point out the best configuration values of parameters for the two-class decision forest for maximum AUC:
    -	Minimum number of samples per leaf node.
    -	Number of random splits per node.
    -	Maximum depth of the decision trees.
    -	Number of decision trees.
5.  If there is enough time, change the parameters of one of the base classifiers, and then rerun the experiment. Remember to change both instances of the base classifier: one on the left and the other on the right side of the model.

>**Note:** This demonstration shows how to build an ensemble of classifiers by using a stacking technique.
>
>**Base classifiers**
>The experiment uses four base classifiers:
>-	**Two-Class Averaged Perceptron**
>-	**Two-Class Decision Forest**
>-	**Two-Class Decision Jungle**
>-	**Two-Class Logistic Regression**
>
>**Ensemble classifier**
>
>In this experiment, the ensemble classifier is logistic regression.
>
>**Datasets**
>
>The datasets included in the Samples Gallery of Machine Learning Studio that are used in this experiment are:
>-	**CRM Dataset Shared**
>-	**CRM Upselling Labels Shared**
>
>**Objective**
>
>Identify a classifier that optimizes AUC.
>
>**Model**
>
>The experiment is built of two main sections. On the right, four base classifiers are built. They are based on the full training dataset to obtain baseline >performance. On the left, the same four classifiers are built into a stacked ensemble. Both the base classifiers and the ensemble of classifiers are tuned for >optimal AUC.
>
>**Results**
>
>At the final step, the experiment generates a result dataset with the **Execute R Script** module. In the table of results, you can compare the AUC performance for each >base classifier and the ensemble.
>
>**Note:** Running the experiment may take over an hour to complete, so you should start the run at least an hour before you start teaching this lesson.


---

©2018 Microsoft Corporation. All rights reserved.

The text in this document is available under the [Creative Commons Attribution 3.0 License](https://creativecommons.org/licenses/by/3.0/legalcode), additional terms may apply. All other content contained in this document (including, without limitation, trademarks, logos, images, etc.) are **not** included within the Creative Commons license grant. This document does not provide you with any legal rights to any intellectual property in any Microsoft product. You may copy and use this document for your internal, reference purposes.

This document is provided "as-is." Information and views expressed in this document, including URL and other Internet Web site references, may change without notice. You bear the risk of using it. Some examples are for illustration only and are fictitious. No real association is intended or inferred. Microsoft makes no warranties, express or implied, with respect to the information provided here.